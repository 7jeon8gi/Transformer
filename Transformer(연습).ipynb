{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "![title](input.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.\n",
    "\n",
    "vocab_length = 1000 # word piece 필요\n",
    "d_model = 8 # 실제로는 128\n",
    "padding_idx = 0 # 문장의 길이를 통일하기 위한 패딩\n",
    "\n",
    "nn_emb = nn.Embedding(num_embeddings = vocab_length, embedding_dim=d_model, padding_idx=padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가상의 텍스트 = \"배 고프다 _ _ _ _ _ _ _. 집에 가고 싶다._ _ _ _ _\"\n",
    "#[[1,23,3,0,0],[4,3,2,55,52]]\n",
    "\n",
    "inputs = torch.LongTensor([[1,23,3,1,1,0,0,0,0,0],[4,3,2,55,52,1,0,0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "inputs_emb = nn_emb(inputs)\n",
    "print(inputs_emb.size()) # 2문장 10토큰 8디멘션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- input------------------------\n",
      "tensor([[  1,  23,  10, 100,   5,   0,   0,   0,   0,   0]])\n",
      "--------------- nn.Embedding ----------------\n",
      "torch.Size([1, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "#\"text1 = 나 밥 먹고 싶 다. 나 도 훌 훌 훌 훌\"\n",
    "print(\"--------------- input------------------------\")\n",
    "text1 = torch.LongTensor([[1,23,10,100,5,0,0,0,0,0]])\n",
    "print(text1)\n",
    "print(\"--------------- nn.Embedding ----------------\")\n",
    "text1 = nn_emb(text1)\n",
    "print(text1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 개념이해\n",
    "1. 일단 시키는대로 짜본다\n",
    "2. 좀 더 로지컬하게 짜본다(구글링)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2665, -1.2355,  0.0866,  1.7763,  0.5019,  0.1455, -1.6850,  0.1170],\n",
       "        [ 1.1478, -0.1244,  0.1221,  0.2425,  1.0318, -1.8287, -0.3031,  1.1272],\n",
       "        [-0.5989, -0.1476,  1.2095, -0.6576, -1.2267, -0.1683,  0.5683,  0.8755],\n",
       "        [ 1.0173,  0.6962,  0.1463,  0.8080, -0.9035,  1.0776,  0.1292,  0.7145],\n",
       "        [-0.9220, -1.6004, -1.2133, -0.0131,  1.5452, -0.7024, -1.3428,  1.4946],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 1., 0., 1., 0., 1.])\n",
      "tensor([8.4147e-01, 9.9500e-01, 9.9998e-03, 1.0000e+00, 1.0000e-04, 1.0000e+00,\n",
      "        1.0000e-06, 1.0000e+00])\n",
      "tensor([9.0930e-01, 9.8007e-01, 1.9999e-02, 1.0000e+00, 2.0000e-04, 1.0000e+00,\n",
      "        2.0000e-06, 1.0000e+00])\n",
      "tensor([1.4112e-01, 9.5534e-01, 2.9996e-02, 1.0000e+00, 3.0000e-04, 1.0000e+00,\n",
      "        3.0000e-06, 1.0000e+00])\n",
      "tensor([-7.5680e-01,  9.2106e-01,  3.9989e-02,  9.9999e-01,  4.0000e-04,\n",
      "         1.0000e+00,  4.0000e-06,  1.0000e+00])\n",
      "tensor([-9.5892e-01,  8.7758e-01,  4.9979e-02,  9.9999e-01,  5.0000e-04,\n",
      "         1.0000e+00,  5.0000e-06,  1.0000e+00])\n",
      "tensor([-2.7942e-01,  8.2534e-01,  5.9964e-02,  9.9998e-01,  6.0000e-04,\n",
      "         1.0000e+00,  6.0000e-06,  1.0000e+00])\n",
      "tensor([6.5699e-01, 7.6484e-01, 6.9943e-02, 9.9998e-01, 7.0000e-04, 1.0000e+00,\n",
      "        7.0000e-06, 1.0000e+00])\n",
      "tensor([9.8936e-01, 6.9671e-01, 7.9915e-02, 9.9997e-01, 8.0000e-04, 1.0000e+00,\n",
      "        8.0000e-06, 1.0000e+00])\n",
      "tensor([4.1212e-01, 6.2161e-01, 8.9879e-02, 9.9996e-01, 9.0000e-04, 1.0000e+00,\n",
      "        9.0000e-06, 1.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "# for 1 sentence\n",
    "a=[]\n",
    "for pos ,v in enumerate(text1[0]): #pos: 0, 1, 2, ... , 9\n",
    "    position = torch.zeros(d_model) # position: 8\n",
    "    for d_i , w in enumerate(v):\n",
    "        if d_i % 2 ==0: #d_i: position8개 중 몇번 째\n",
    "            PE = np.sin(pos / np.power(10000, (2* d_i / d_model)))\n",
    "            position[d_i] += PE\n",
    "        else:\n",
    "            PE = np.cos(pos / np.power(10000, (2* d_i / d_model)))\n",
    "            position[d_i] +=PE\n",
    "            \n",
    "    print(position)\n",
    "    a.append(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2665e+00, -2.3554e-01,  8.6642e-02,  2.7763e+00,  5.0190e-01,\n",
      "          1.1455e+00, -1.6850e+00,  1.1170e+00],\n",
      "        [ 1.9893e+00,  8.7059e-01,  1.3210e-01,  1.2425e+00,  1.0319e+00,\n",
      "         -8.2874e-01, -3.0314e-01,  2.1272e+00],\n",
      "        [ 3.1042e-01,  8.3250e-01,  1.2294e+00,  3.4236e-01, -1.2265e+00,\n",
      "          8.3170e-01,  5.6831e-01,  1.8755e+00],\n",
      "        [ 1.1584e+00,  1.6516e+00,  1.7628e-01,  1.8080e+00, -9.0322e-01,\n",
      "          2.0776e+00,  1.2915e-01,  1.7145e+00],\n",
      "        [-1.6788e+00, -6.7931e-01, -1.1734e+00,  9.8688e-01,  1.5456e+00,\n",
      "          2.9759e-01, -1.3428e+00,  2.4946e+00],\n",
      "        [-9.5892e-01,  8.7758e-01,  4.9979e-02,  9.9999e-01,  5.0000e-04,\n",
      "          1.0000e+00,  5.0000e-06,  1.0000e+00],\n",
      "        [-2.7942e-01,  8.2534e-01,  5.9964e-02,  9.9998e-01,  6.0000e-04,\n",
      "          1.0000e+00,  6.0000e-06,  1.0000e+00],\n",
      "        [ 6.5699e-01,  7.6484e-01,  6.9943e-02,  9.9998e-01,  7.0000e-04,\n",
      "          1.0000e+00,  7.0000e-06,  1.0000e+00],\n",
      "        [ 9.8936e-01,  6.9671e-01,  7.9915e-02,  9.9997e-01,  8.0000e-04,\n",
      "          1.0000e+00,  8.0000e-06,  1.0000e+00],\n",
      "        [ 4.1212e-01,  6.2161e-01,  8.9879e-02,  9.9996e-01,  9.0000e-04,\n",
      "          1.0000e+00,  9.0000e-06,  1.0000e+00]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.stack(a)\n",
    "Input_embed = a+ text1[0]\n",
    "print(Input_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nested function 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional(sequence):\n",
    "    def calculate(position, d_i):\n",
    "        return position / np.power(10000, (2* d_i / d_model))\n",
    "    def PE(position):\n",
    "        return [calculate(position, d_i) for d_i in range(d_model)] #0,1, 2, ... ,127\n",
    "    \n",
    "    sinusoid_table = np.array([PE(seq) for seq in range(sequence)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:,0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:,1::2])\n",
    "    \n",
    "    return torch.from_numpy(sinusoid_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seq = 8\n",
    "pos_encoding = positional(n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  9.9500e-01,  9.9998e-03,  1.0000e+00,  1.0000e-04,\n",
       "          1.0000e+00,  1.0000e-06,  1.0000e+00],\n",
       "        [ 9.0930e-01,  9.8007e-01,  1.9999e-02,  1.0000e+00,  2.0000e-04,\n",
       "          1.0000e+00,  2.0000e-06,  1.0000e+00],\n",
       "        [ 1.4112e-01,  9.5534e-01,  2.9996e-02,  1.0000e+00,  3.0000e-04,\n",
       "          1.0000e+00,  3.0000e-06,  1.0000e+00],\n",
       "        [-7.5680e-01,  9.2106e-01,  3.9989e-02,  9.9999e-01,  4.0000e-04,\n",
       "          1.0000e+00,  4.0000e-06,  1.0000e+00],\n",
       "        [-9.5892e-01,  8.7758e-01,  4.9979e-02,  9.9999e-01,  5.0000e-04,\n",
       "          1.0000e+00,  5.0000e-06,  1.0000e+00],\n",
       "        [-2.7942e-01,  8.2534e-01,  5.9964e-02,  9.9998e-01,  6.0000e-04,\n",
       "          1.0000e+00,  6.0000e-06,  1.0000e+00],\n",
       "        [ 6.5699e-01,  7.6484e-01,  6.9943e-02,  9.9998e-01,  7.0000e-04,\n",
       "          1.0000e+00,  7.0000e-06,  1.0000e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
       "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1부터 시작으로 바꾸자\n",
    "position = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous()+ 1\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5, 0, 0, 0, 0, 0],\n",
       "        [1, 2, 3, 4, 5, 6, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position.masked_fill_(inputs.eq(0), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(8, 8)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_pos = nn.Embedding.from_pretrained(pos_encoding, freeze=True)\n",
    "nn_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 8.4147e-01,  9.9500e-01,  9.9998e-03,  1.0000e+00,  1.0000e-04,\n",
       "           1.0000e+00,  1.0000e-06,  1.0000e+00],\n",
       "         [ 9.0930e-01,  9.8007e-01,  1.9999e-02,  1.0000e+00,  2.0000e-04,\n",
       "           1.0000e+00,  2.0000e-06,  1.0000e+00],\n",
       "         [ 1.4112e-01,  9.5534e-01,  2.9996e-02,  1.0000e+00,  3.0000e-04,\n",
       "           1.0000e+00,  3.0000e-06,  1.0000e+00],\n",
       "         [-7.5680e-01,  9.2106e-01,  3.9989e-02,  9.9999e-01,  4.0000e-04,\n",
       "           1.0000e+00,  4.0000e-06,  1.0000e+00],\n",
       "         [-9.5892e-01,  8.7758e-01,  4.9979e-02,  9.9999e-01,  5.0000e-04,\n",
       "           1.0000e+00,  5.0000e-06,  1.0000e+00],\n",
       "         [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
       "\n",
       "        [[ 8.4147e-01,  9.9500e-01,  9.9998e-03,  1.0000e+00,  1.0000e-04,\n",
       "           1.0000e+00,  1.0000e-06,  1.0000e+00],\n",
       "         [ 9.0930e-01,  9.8007e-01,  1.9999e-02,  1.0000e+00,  2.0000e-04,\n",
       "           1.0000e+00,  2.0000e-06,  1.0000e+00],\n",
       "         [ 1.4112e-01,  9.5534e-01,  2.9996e-02,  1.0000e+00,  3.0000e-04,\n",
       "           1.0000e+00,  3.0000e-06,  1.0000e+00],\n",
       "         [-7.5680e-01,  9.2106e-01,  3.9989e-02,  9.9999e-01,  4.0000e-04,\n",
       "           1.0000e+00,  4.0000e-06,  1.0000e+00],\n",
       "         [-9.5892e-01,  8.7758e-01,  4.9979e-02,  9.9999e-01,  5.0000e-04,\n",
       "           1.0000e+00,  5.0000e-06,  1.0000e+00],\n",
       "         [-2.7942e-01,  8.2534e-01,  5.9964e-02,  9.9998e-01,  6.0000e-04,\n",
       "           1.0000e+00,  6.0000e-06,  1.0000e+00],\n",
       "         [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embs = nn_pos(position)\n",
    "pos_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 8]) torch.Size([2, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "print(pos_embs.shape,inputs_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_sums = pos_embs + inputs_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = 1000 # word piece 필요\n",
    "d_model = 8 # 실제로는 512\n",
    "#sequence_length = ??\n",
    "padding_idx = 0 # 문장의 길이를 통일하기 위한 패딩\n",
    "layer_length = 6\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_embedding = nn.Embedding(vocab_length, d_model, padding_idx = 0)\n",
    "        sinusoid_table = positional(sequence_length)\n",
    "        self.position_encoding = nn.Embedding.from_pretrained(sinusoid_table)\n",
    "        #self.layers = nn.ModuleList([])\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Mask for Positional Encoding\n",
    "        positions = torch.arange(inputs.size(1), dtype=inputs.dtype).expand(inputs.size(0),inputs.size(1)).contiguous()+1\n",
    "        position_mask = inquts.eq(0)\n",
    "        positions.masked_fill_(position_maks,0)\n",
    "        \n",
    "        outpus = self.input_embedding(inputs) + self.position_encoding(positions)\n",
    "        \n",
    "        #Encoder Layer loop layer_lenght times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
